{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import Subset\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from typing import *\n",
    "\n",
    "\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import albumentations as A\n",
    "import torchvision.transforms as ts\n",
    "\n",
    "\n",
    "ROOT_TRAIN = \"data/train\"\n",
    "ROOT_VAL = \"data/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    # Фискирует максимум сидов.\n",
    "    # Это понадобится, чтобы сравнение различных моделей было корректным\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything(123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>ML Project 2. Image classification</h4>\n",
    "\n",
    "\n",
    "<h4>School of Quants. ML2 project. Tiny Image classification</h4>\n",
    "\n",
    "<a href=\"https://github.com/B0R0koko/TinyImageClassification200_SoQ_ML2\">Github repo</a>\n",
    "\n",
    "\n",
    "<p>In this project we trained EfficientNet model for image classification (Tiny Image Dataset)</p>\n",
    "<br/>\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Index</th>\n",
    "    <th>Model</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>EfficientNetb4_64x64_locally_trained</td>\n",
    "    <td>0.405</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>EfficientNetb4_224x224_ImageNet_pretrained_tuned</td>\n",
    "    <td>0.755</td>\n",
    "  </tr>\n",
    "</table>\n",
    "<br/>\n",
    "<ul>\n",
    "    <li>As the first part, we trained untrained EfficientNet model from scratch and get the accuracy on validation of 0.404. Grade - 9.02 / 10</li>\n",
    "    <li>As a second part of the task, we trained a pretrained EfficientNet model on Imagenet 1k dataset, we added the last layer to make the last dense layer output 200 values which matches the number of labels in the dataset. With this, we were able to achieve 0.755 accuracy on validation set, which is shy of the target 0.84, but still good enough. Grade 7.53 / 10</li>\n",
    "</ul>\n",
    "\n",
    "<p>Unpack zipped data file into data folder with train and val folders. Also in the log folder we have log files which tracked the progress of the training and validation. We used Tensorboard for tracking.</p>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"./imgs/image.png\" alt=\"Tensorboard. Model 1\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Task 1</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders for train and val\n",
    "\n",
    "train_transform = Compose([\n",
    "    ts.RandomRotation(degrees=20),\n",
    "    ts.RandomHorizontalFlip(p=0.2),\n",
    "    ts.ToTensor(),\n",
    "    ts.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    ts.ToTensor(),\n",
    "    ts.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = ImageFolder(ROOT_TRAIN, transform=train_transform)\n",
    "val_dataset = ImageFolder(ROOT_VAL, transform=val_transform)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, pin_memory=True, batch_size=128, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, pin_memory=True, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show a couple of pictures\n",
    "for image, label in val_dataloader:\n",
    "\n",
    "    image = image[0].squeeze()\n",
    "    \n",
    "    # Transpose the dimensions to match the standard image dimensions (64, 64, 3)\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Create Model and Trainer class</h4>\n",
    "\n",
    "<p>Here we are training EfficientNet_b4 model. I tried almost all architectures listed on Pytorch website and also attempted to apply multiple different transformations\n",
    "but still failed to reach the target of 0.44 accuracy, but we were able to get very close to it with the max validation score of 0.405</p>\n",
    "\n",
    "<p>Class structure and code snippets were taken from another course I am taking at university</p>\n",
    "\n",
    "<a href=\"https://github.com/fintech-dl-hse/course\">Github of the course</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from torch import Tensor\n",
    "from torchvision.models import efficientnet_b4\n",
    "\n",
    "class Model(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_batch(self, X_batch: torch.Tensor, label: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"forward batch and compute evaluation metrics\"\"\"\n",
    "\n",
    "\n",
    "class MyNet(Model, nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> Self:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.model = efficientnet_b4(weights=None, num_classes=200)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def compute_batch(self, X_batch: Tensor, label: Tensor) -> Tuple[torch.Tensor, float]:\n",
    "        \n",
    "        logits: torch.Tensor = self.forward(X_batch)\n",
    "        loss: torch.Tensor = F.cross_entropy(logits, label)\n",
    "        # compare to ground truth\n",
    "        accuracy: np.array[bool] = (logits.argmax(axis=1) == label).float().mean().cpu().numpy()\n",
    "\n",
    "        return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define trainer class for training and validation of the model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, model: Model, optimizer, train_loader: DataLoader, \n",
    "        val_loader: DataLoader, log_dir=\"log\"\n",
    "    ) -> Self:\n",
    "        \n",
    "        self.model: Model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.device: str = torch.cuda.current_device() if torch.cuda.is_available() else \"cpu\"\n",
    "        # Train model on CUDA device or CPU\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        # Create dataloaders for train and validation splits\n",
    "        self.train_loader: DataLoader = train_loader\n",
    "        self.val_loader: DataLoader = val_loader\n",
    "\n",
    "        # Log training process to Tensorboard\n",
    "        self.logger: SummaryWriter = SummaryWriter(log_dir=log_dir)\n",
    "        self.global_step: int = 0\n",
    "\n",
    "\n",
    "    def train_one_epoch(self) -> None:\n",
    "        # Set model into train mode \n",
    "        self.model.train()\n",
    "\n",
    "        X_batch: torch.Tensor\n",
    "        label: torch.Tensor\n",
    "\n",
    "        train_accuracies: List[float] = []\n",
    "\n",
    "        # Iterating over batches\n",
    "        for X_batch, label in tqdm(self.train_loader):\n",
    "            X_batch = X_batch.to(self.device) \n",
    "            label = label.to(self.device)\n",
    "\n",
    "            train_loss: torch.Tensor\n",
    "            train_accuracy: float\n",
    "            train_loss, train_accuracy = self.model.compute_batch(X_batch=X_batch, label=label)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Take average train_accuracy across batches\n",
    "        train_accuracy = np.mean(train_accuracies)\n",
    "        self.logger.add_scalar(tag=\"train_accuracy\", scalar_value=train_accuracy, global_step=self.global_step)\n",
    "\n",
    "        val_accuracies: List[float] = []\n",
    "        # Iterate over val data\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "\n",
    "            for X_batch, label in tqdm(self.val_loader):\n",
    "                X_batch = X_batch.to(self.device) \n",
    "                label = label.to(self.device)\n",
    "\n",
    "                val_loss: torch.Tensor\n",
    "                val_accuracy: float\n",
    "                val_loss, val_accuracy = self.model.compute_batch(X_batch=X_batch, label=label)\n",
    "                val_accuracies.append(val_accuracy)\n",
    "\n",
    "        val_accuracy = np.mean(val_accuracies)\n",
    "        self.logger.add_scalar(tag=\"val_accuracy\", scalar_value=val_accuracy, global_step=self.global_step)\n",
    "\n",
    "        self.global_step += 1\n",
    "\n",
    "\n",
    "    def train(self, num_epochs: int) -> None:\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train_one_epoch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Start training process</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load trained model (my own checkpoint)\n",
    "model = MyNet()\n",
    "model.load_state_dict(torch.load(\"trained_models/model_task1.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNet()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, optimizer=optimizer, train_loader=train_dataloader, val_loader=val_dataloader,\n",
    "    log_dir=\"log/log_model1\"\n",
    ")\n",
    "\n",
    "trainer.train(num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I use the code snippet provided in the file to evaluate the model and grade it\n",
    "def evaluate_task(model: Model, val_dataloader: DataLoader, device=\"cuda:0\"):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    val_accuracies = []\n",
    "\n",
    "    for images, labels in tqdm(val_dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits: torch.Tensor = model.forward(images)\n",
    "            # compare to ground truth\n",
    "            accuracy: np.array[bool] = (logits.argmax(axis=1) == labels).float().mean().cpu().numpy()\n",
    "            val_accuracies.append(accuracy)\n",
    "    \n",
    "    return np.mean(val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:06<00:00, 12.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оценка за это задание составит 9.02 баллов\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_accuracy: float = evaluate_task(model=model, val_dataloader=val_dataloader)\n",
    "\n",
    "print(f\"Оценка за это задание составит {np.clip(10 * val_accuracy / 0.44, 0, 10):.2f} баллов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "os.makedirs(\"trained_models\", exist_ok=True)\n",
    "\n",
    "torch.save(model.state_dict(), \"trained_models/model_task1.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Task 2. Reach 0.84 accuracy on val set</h4>\n",
    "\n",
    "<p>We will perform transfer learning, we will use EfficientNet b4 pretrained on 1k Imagenet dataset, we will slightly adjust the last layer to match 200 labels in this dataset</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = ts.Compose([\n",
    "    ts.Resize(size=(224, 224)),\n",
    "    ts.RandomRotation(20),\n",
    "    ts.RandomHorizontalFlip(0.5),\n",
    "    ts.ToTensor(),\n",
    "    ts.Normalize([0.4802, 0.4481, 0.3975], [0.2302, 0.2265, 0.2262]),\n",
    "])\n",
    "\n",
    "val_transform = ts.Compose([\n",
    "    ts.Resize(size=(224, 224)),\n",
    "    ts.ToTensor(),\n",
    "    ts.Normalize([0.4802, 0.4481, 0.3975], [0.2302, 0.2265, 0.2262]),\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = ImageFolder(ROOT_TRAIN, transform=train_transform)\n",
    "val_dataset = ImageFolder(ROOT_VAL, transform=val_transform)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset, pin_memory=True, batch_size=32, shuffle=True\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=val_dataset, pin_memory=True, batch_size=32, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Transfer learning + Resizing</h4>\n",
    "\n",
    "<p>Use weights of the pretrained model and modify the last layer to match output size</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import swin_v2_b, Swin_V2_B_Weights\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.models import EfficientNet_B4_Weights, efficientnet_b4\n",
    "\n",
    "\n",
    "class MyNet2(Model, nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> Self:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.inner = nn.Sequential(\n",
    "            efficientnet_b4(weights=EfficientNet_B4_Weights.IMAGENET1K_V1),\n",
    "            nn.Linear(1000, 200)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.inner(x)\n",
    "    \n",
    "    def compute_batch(self, X_batch: Tensor, label: Tensor) -> Tuple[torch.Tensor, float]:\n",
    "        \n",
    "        logits: torch.Tensor = self.forward(X_batch)\n",
    "        loss: torch.Tensor = F.cross_entropy(logits, label)\n",
    "        # compare to ground truth\n",
    "        accuracy: np.array[bool] = (logits.argmax(axis=1) == label).float().mean().cpu().numpy()\n",
    "\n",
    "        return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MyNet2()\n",
    "\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model2, optimizer=optimizer, train_loader=train_dataloader, val_loader=val_dataloader, log_dir=\"log/log_model_2\"\n",
    ")\n",
    "\n",
    "trainer.train(num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"trained_models\", exist_ok=True)\n",
    "\n",
    "torch.save(model2.state_dict(), \"trained_models/model_task2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = MyNet2()\n",
    "model2.load_state_dict(torch.load(\"trained_models/model_task2.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:23<00:00, 13.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оценка за это задание составит 7.53 баллов\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate_task(model2, val_dataloader)\n",
    "print(f\"Оценка за это задание составит {np.clip(10 * (accuracy - 0.5) / 0.34, 0, 10):.2f} баллов\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
